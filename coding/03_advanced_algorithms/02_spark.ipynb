{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark - Map Reduce Beispiel\n",
    "\n",
    "\n",
    "Nachdem wir eingangs MapReduce in Python implementiert haben, wollen wir das Ganze nun nochmal in Apache Spark implementieren.\n",
    "\n",
    "Hierfür muss zunächst Spark lokal installiert werden:\n",
    "* [für Mac](https://sparkbyexamples.com/spark/install-apache-spark-on-mac/)\n",
    "* [für Windows](https://sparkbyexamples.com/spark/apache-spark-installation-on-windows/)\n",
    "\n",
    "Diese Lokalinstallation ist nötig, um Spark-Workloads ausführen zu können (ähnlich dem gehosteten Spark auf z.B. AWS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"WordCountExample\")\n",
    "\n",
    "# Load a text file from your local filesystem into an RDD (Resilient Distributed Dataset)\n",
    "text_file = sc.textFile(\"./input_text.txt\")\n",
    "\n",
    "# Split each line into words\n",
    "words = text_file.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Assign a count of 1 to each word\n",
    "word_counts = words.map(lambda word: (word, 1))\n",
    "\n",
    "# Perform a reduce operation to count the occurrences of each word\n",
    "word_count = word_counts.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Print the word count results\n",
    "for (word, count) in word_count.collect():\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe: Berechnung der Quadratwerte mit PySpark\n",
    "\n",
    "Erstelle ein PySpark-Programm, das eine Liste von Zahlen in einem RDD  (Resilient Distributed Dataset) erstellt und dann die Quadratwerte dieser Zahlen berechnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Erstellen eines SparkContext\n",
    "sc = SparkContext(\"local\", \"SimplePySparkTask\")\n",
    "\n",
    "# Erstellen eines RDD mit einer Liste von Zahlen\n",
    "numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('hector')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "442f59ff6058100981594e9535d28b46fc243840058aa6758dcff500d21b0d70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
